---
title: Insert title here
key: 7774a6cccae2fd3fe8d17311854e7472

---
## Screencast

```yaml
type: "TitleSlide"
key: "be5c7a4595"
```

`@lower_third`

name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL


`@script`
In this lesson, we cover a popular association rule mining technique, the apriori algorithm.


---
## Association rule mining

```yaml
type: "FullSlide"
key: "c825ec5187"
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: from the above frequent itemsets, generate association rules with confidence above a _minimum confidence threshold_.

&nbsp;

The **Apriori algorithm** is a classic and fast mining algorithm belonging to the class of association rule mining algorithms.


`@script`
To extract interesting rules from the transactional dataset, the association rule mining can be decomposed into two subtasks. First, the set of frequent items is generated (the support satisfies a minimum threshold value). Second, rules with high confidence are extracted from this set of frequent itemsets.


---
## Idea behind the Apriori algorithm

```yaml
type: "FullSlide"
key: "0a6b566e2c"
```

`@part1`
**The apriori algorithm:**

- Bottom-up approach
- Generates candidate itemsets by exploiting the apriori principle

**Apriori principle**:

- If an itemset is frequent, then all of its subsets must also be frequent.
	- _e.g._ if {A,B} is frequent, then both {A} and {B} are frequent
- For an infrequent itemset, all its super-sets are infrequent.
	- _e.g._ if {A} is infrequent, then {A,B}, {A,C} and {A,B,C} are infrequent.


`@citations`
Agrawal and Srikant (1994)


`@script`
The Apriori algorithm uses a bottom-up approach, where frequent subsets are extended one item at a time. This step is known as candidate generation, and groups of candidates are tested again the dataset.

The algorithm makes use of the apriori principle to generate candidate itemsets efficiently: 

- If an itemset such as {A,B} is frequent, then both subsets {A} and {B} are frequent.
- Likewise, if an itemset such as {A} is infrequent, then all its super-sets such as {A,B}, {A,C} and {A,B,C} are infrequent.


---
## Example: 1-itemset

```yaml
type: "TwoColumns"
key: "867fc1190a"
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
To illustrate how the apriori algorithm works, we use the previous transactional dataset and the corresponding itemset lattice. 
 
The algorithm starts by counting the number of occurences of A; in this case it appears 3 times. Similarly, by scanning the dataset, items B,C and D are flagged as frequent items given they all satisfy the minimum support threshold. These frequent 1-itemsets are used to generate candidate 2-itemsets.


---
## Example: 2-itemsets

```yaml
type: "TwoColumns"
key: "0d9dabb41c"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
For the 2-itemsets, neither (AC) nor (AD) satisfy the minimum support threshold. This means from the apriori principle that all their supersets are neither frequent itemsets.


---
## Example: 3-itemsets

```yaml
type: "TwoColumns"
key: "5608bd3d6f"
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)


`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Both infrequent itemsets (AC) and (AD) and their respective super-sets are colored in orange on the graph. The exploration space has therefore been substantially reduced.

The remaining 3-itemset candidate, the itemset {BCD}, is infrequent as its support is 2.


---
## Example: frequent itemsets

```yaml
type: "TwoColumns"
key: "bc082b2107"
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42


`@citations`
Minimum support threshold = 3/7 = 0.42


`@script`
Finally, all frequent itemsets are shown in blue on the graph and displayed in the table.


---
## Apriori: rule generation

```yaml
type: "FullSlide"
key: "4bb5e1a333"
```

`@part1`
After the computationally expensive frequent itemset generation, apriori generates rules:

- Start with high-confidence rules with single precedent 
	- _e.g._ {A,C} => {B}
- Build more complex rules, with more items on the right hand side 
	- _e.g._ {A,C} => {B, D}

&nbsp;

**Trick**: pruning of association rule

_e.g._: if the rule {BCD} => {A} has low confidence, all rules containing item A in its consequent can be discarded (such as the rule {B,D} => {A, C} or  {D} => {A,B, C}).


`@script`
After retrieving frequent itemsets (which is the most computationally intense part of the algorithm), the apriori algorithm generates associated rules.

It starts with rules containing a single item in the rule consequent and generates more complex candidate rules in an efficient manner.
For instance, if rule (BCD) implies A has low confidence, then all rules with item A in its consequent can be discarded.


---
## A first try with the apriori

```yaml
type: "FullSlide"
key: "a6ae55fdc5"
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```


`@script`
To retrieve the list of frequent itemsets in R, we start with a standard call of the apriori function. The arguments are the transactional dataset as well as the following parameters: 1. the minimum support threshold denoted as "supp" and 2. the "target" as frequent itemset.


---
## Output of the apriori - frequent itemsets

```yaml
type: "TwoColumns"
key: "a9f5eecee4"
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```


`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)


`@script`
By inspecting the object, we obtain a dataframe containing the set of frequent itemsets which corresponds to the itemset lattice we built earlier.


---
## Extracting rules with the apriori function

```yaml
type: "FullSlide"
key: "300fe878ef"
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

**Call to the apriori function for rule generation with specific arguments**
```
rules.all = apriori(trans,
				parameter = list(supp=3/7, conf=0.6, minlen=2),
                control = list(verbose=F),
                appearance = list(rhs="A", default = "lhs"))
inspect(rules.all)
```


`@script`
To generate association rules and steer the output of extracted rules, different arguments can be used. The parameter argument contains values such as support, confidence threshold and minimum length of rules. The control argument allows to influence the performance of the algorithm. Appearance allows to specify constraints on the right or left hand side of the generated rules (such as A on the right hand side).


---
## Extracting rules: output

```yaml
type: "FullSlide"
key: "7acad20eeb"
```

`@part1`
**Inspecting the rules** 

```
> inspect(rules.all)
    lhs    rhs support   confidence lift      count
[1] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[2] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[3] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[4] {C} => {B} 0.4285714 0.7500000  0.8750000 3    
[5] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[6] {B} => {D} 0.5714286 0.6666667  0.9333333 4 
```


`@script`
The "inspect" function allows to extract the generate rules with their corresponding measures.
For instance, the first rule states that "A implies B" for a support of 42% and a confidence of 100%.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "d9c318cd97"
```

`@part1`
**Set of generated rules**

```
rules = apriori(trans,control = list(verbose=F),
                parameter = list(supp=0.05, conf=0.5, minlen=2),
                appearance = list(rhs="A", default = "lhs"))
 
```

**Set of pruned rules (non-redundant)**

```
subset.mat = is.subset(rules, rules)
redundant = colSums(subset.mat)>1
rules.pruned = rules[!redundant]
```


`@script`
There are often too many association rules discovered from a transactional dataset. Some rules may be redundant with respect to other extracted rules.

We use the "is.subset" function to figure out which rules are redundant and exclude these from the original set of extracted rules.


---
## Rule redundancy

```yaml
type: "FullSlide"
key: "3a81639fe1"
```

`@part1`
**Comparing extracted rules and non-redundant rules**

```
> inspect(rules) 
    lhs        rhs support   confidence lift     count
[1] {B}     => {A} 0.4285714 0.5        1.166667 3    
[2] {B,D}   => {A} 0.2857143 0.5        1.166667 2    
[3] {B,C,D} => {A} 0.1428571 0.5        1.166667 1 
```

```
> inspect(rules.pruned)
    lhs    rhs support   confidence lift     count
[1] {B} => {A} 0.4285714 0.5        1.166667 3 
```


`@script`
In this example, rule #2 and rule #3 are considered redundant rules as they provide no extra knowledge in addition to rule #1.
"B implies A" is a non-redundant rule given that:
- rules #2 and #3 are super-rules of that rule
- they both have the same or lower lift


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "cbc4d2b5e4"
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. Happy shopping!

