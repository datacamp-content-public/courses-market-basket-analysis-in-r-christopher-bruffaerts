---
title: 'Insert title here'
key: 7774a6cccae2fd3fe8d17311854e7472
---

## Screencast

```yaml
type: TitleSlide
key: be5c7a4595
```

`@lower_third`
name: Christopher Bruffaerts
title: Lecturer in Data science, EPFL

`@script`
In this lesson, we cover a popular association rule mining technique, the apriori algorithm.

---

## Association rule mining

```yaml
type: FullSlide
key: c825ec5187
```

`@part1`
**Association rule mining** allows to discover interesting relationships between items in a large transactional database.

This mining task can be divided into two subtasks:

- **Frequent itemset generation**: determine all frequent itemsets of a potentially large database of transactions. An itemset is said to be frequent if it satisfies a _minimum support threshold_.

- **Rule generation**: from the above frequent itemsets, generate association rules with confidence above a _minimum confidence threshold_.

&nbsp;

The **Apriori algorithm** is a classic and fast mining algorithm belonging to the class of association rule mining algorithms.

`@script`
To extract interesting rules from the transactional dataset, the association rule mining can be decomposed into two subtasks. First, the set of frequent items is generated (items for which the support satisfies a minimum threshold value). Second, rules with high confidence are extracted from this set of frequent itemsets.

---

## Idea behind the Apriori algorithm

```yaml
type: FullSlide
key: 0a6b566e2c
```

`@part1`
**The apriori algorithm:**

- Bottom-up approach
- Generates candidate itemsets by exploiting the apriori principle

**Apriori principle**:

- If an itemset is frequent, then all of its subsets must also be frequent.
	- _e.g._ if {A,B} is frequent, then both {A} and {B} are frequent
- For an infrequent itemset, all its super-sets are infrequent.
	- _e.g._ if {A} is infrequent, then {A,B}, {A,C} and {A,B,C} are infrequent.

`@script`
The Apriori algorithm uses a bottom-up approach, where frequent subsets are extended one item at a time. This step is known as candidate generation, and groups of candidates are tested again the dataset.

The algorithm makes use of the apriori principle to generate candidate itemsets efficiently: 

- If an itemset such as {A,B} is frequent, then both subsets {A} and {B} are frequent.
- Likewise, if an itemset such as {A} is infrequent, then all its super-sets such as {A,B}, {A,C} and {A,B,C} are infrequent.

`@citations`
Agrawal and Srikant (1994)

---

## Example: 1-itemset

```yaml
type: TwoColumns
key: 867fc1190a
```

`@part1`
![item_set_lattice2](https://assets.datacamp.com/production/repositories/4926/datasets/2410e69c0cc858c2d970f83b1d36b98daf3f7b6e/itemset_lattice_1_r.png =80)

`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7     | {B, D}         |

`@script`
To illustrate how the apriori algorithm works, we use the previous transactional dataset and the corresponding itemset lattice. 
 
The algorithm starts by counting the number of occurences of A; in this case it appears 3 times. Similarly, by scanning the dataset, items B,C and D are flagged as frequent items given they all satisfy the minimum support threshold. These frequent 1-itemsets are used to generate candidate 2-itemsets.

`@citations`
Minimum support threshold = 3/7 = 0.42

---

## Example: 2-itemsets

```yaml
type: TwoColumns
key: 0d9dabb41c
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/c47cab96e3fe4710525fcece8c9299b86feb52b7/itemset_lattice_2_r.png =80)

`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |

`@script`
For the 2-itemsets, neither itemsets (AC) nor (AD) satisfy the minimum support threshold. This means from the apriori principle that all their supersets are neither frequent itemsets.

`@citations`
Minimum support threshold = 3/7 = 0.42

---

## Example: 3-itemsets

```yaml
type: TwoColumns
key: 5608bd3d6f
```

`@part1`
![all_itemsets](https://assets.datacamp.com/production/repositories/4926/datasets/a95bdc27e475bdb72214862125361e91c975bb48/itemset_lattice_2b_r.png =75)

`@part2`
| TID 	 | Transaction                            |
|--------|----------------------------------------|
| 1      | {A, B, C, D}     	  |
| 2      | {A, B, D}              |
| 3      | {A, B}                |
| 4      | {B, C, D}         |
| 5      | {B, C}         |
| 6      | {C, D}         |
| 7      | {B, D}         |

`@script`
Both infrequent itemsets (AC) and (AD) and their respective super-sets are colored in orange on the graph. The exploration space has therefore been substantially reduced.

The remaining 3-itemset candidate, the itemset {BCD}, is infrequent as its support is 2.

`@citations`
Minimum support threshold = 3/7 = 0.42

---

## Example: frequent itemsets

```yaml
type: TwoColumns
key: bc082b2107
```

`@part1`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)

`@part2`
| Itemset| Count  | Support |
|--------|--------|---------|
| {A}    | 3      | 0.42
| {B}    | 6      | 0.85
| {C}    | 4      | 0.57
| {D}    | 5      | 0.71
| {A,B}  | 3 	  | 0.42
| {B,C}  | 3      | 0.42
| {B,D}  | 4      | 0.57
| {C,D}  | 3      | 0.42

`@script`
Finally, all frequent itemsets are shown in blue on the graph and displayed in the table.

`@citations`
Minimum support threshold = 3/7 = 0.42

---

## Apriori: rule generation

```yaml
type: FullSlide
key: 4bb5e1a333
```

`@part1`
After the computationally expensive frequent itemset generation, apriori generates rules:

- Start with high-confidence rules with single precedent 
	- _e.g._ {A,C} => {B}
- Build more complex rules, with more items on the right hand side 
	- _e.g._ {A,C} => {B, D}

&nbsp;

**Trick**: pruning of association rule

_e.g._: if the rule {BCD} => {A} has low confidence, all rules containing item A in its consequent can be discarded (such as the rule {B,D} => {A, C} or  {D} => {A,B, C}).

`@script`
After the expensive step of retrieving frequent itemsets, the apriori generates associated rules.

It starts with rules containing a single item in the rule consequent to generate later more complex candidate rules in an efficient manner.
For instance, if the rule (BCD) implies A has low confidence, then all rules with item A in its consequent are discarded/pruned.

---

## A first try with the apriori

```yaml
type: FullSlide
key: a6ae55fdc5
```

`@part1`
**Transactional data**
```
inspect(head(trans,2))
```

```
    items     transactionID
[1] {A,B,C,D} 1            
[2] {A,B,D}   2            
```

**First call to the apriori function - frequent itemsets**
```r
support.all = apriori(trans, 
                      parameter = list(supp = 3/7, target="frequent itemsets"))
```

`@script`
To retrieve the list of frequent itemsets in R, we use the apriori function. Arguments are the transactional dataset and the parameter containing: 1. the minimum support threshold denoted as "supp" and 2. the "target" as frequent itemsets.

---

## Output of the apriori - frequent itemsets

```yaml
type: TwoColumns
key: a9f5eecee4
```

`@part1`
**Frequent itemsets**
```
> inspect(support.all)
    items support   count
[1] {A}   0.4285714 3    
[2] {C}   0.5714286 4    
[3] {D}   0.7142857 5    
[4] {B}   0.8571429 6    
[5] {A,B} 0.4285714 3    
[6] {C,D} 0.4285714 3    
[7] {B,C} 0.4285714 3    
[8] {B,D} 0.5714286 4
```

`@part2`
![item_set_lattice3](https://assets.datacamp.com/production/repositories/4926/datasets/dd85eef5519ce0bde80f9cf799c167d939b2a563/itemset_lattice_3_r.png =78)

`@script`
By inspecting the object, we obtain a dataframe containing the set of frequent itemsets which corresponds to the itemset lattice we built earlier.

---

## Extracting rules with the apriori function

```yaml
type: FullSlide
key: 300fe878ef
```

`@part1`
**Parameter**: the mining parameters change the characteristics of the mined itemsets or rules.
- Support = 3/7
- Confidence = 60%
- Minimum length of rule = 2

**Call to the apriori function for rule generation with specific arguments**
```
rules.all = apriori(trans,
				parameter = list(supp=3/7, conf=0.6, minlen=2),
                control = list(verbose=F),
                appearance = list(rhs="A", default = "lhs"))
inspect(rules.all)
```

`@script`
To generate association rules and steer the output of extracted rules, different arguments can be used. The parameter argument contains values such as support, confidence threshold and minimum length of rules. The control argument allows to influence the performance of the algorithm. Appearance allows to specify constraints on the right or left hand side of the generated rules (such as A on the right hand side).

---

## Extracting rules: output

```yaml
type: FullSlide
key: 7acad20eeb
```

`@part1`
**Inspecting the rules** 

```
> inspect(rules.all)
    lhs    rhs support   confidence lift      count
[1] {A} => {B} 0.4285714 1.0000000  1.1666667 3    
[2] {C} => {D} 0.4285714 0.7500000  1.0500000 3    
[3] {D} => {C} 0.4285714 0.6000000  1.0500000 3    
[4] {C} => {B} 0.4285714 0.7500000  0.8750000 3    
[5] {D} => {B} 0.5714286 0.8000000  0.9333333 4    
[6] {B} => {D} 0.5714286 0.6666667  0.9333333 4 
```

`@script`
We inspect the object to extract the generate rules with their corresponding measures.
For instance, the first rule states that "A implies B" for a support of 42% and a confidence of 100%.

---

## Rule redundancy

```yaml
type: FullSlide
key: d9c318cd97
```

`@part1`
**Set of generated rules**

```
rules = apriori(trans,control = list(verbose=F),
                parameter = list(supp=0.05, conf=0.5, minlen=2),
                appearance = list(rhs="A", default = "lhs"))
 
```

**Set of pruned rules (non-redundant)**

```
subset.mat = is.subset(rules, rules)
redundant = colSums(subset.mat)>1
rules.pruned = rules[!redundant]
```

`@script`
There are often too many association rules discovered from a transactional dataset. Some rules may be redundant with respect to other extracted rules.

We use the "is.subset" function to figure out which rules are redundant and exclude these from the original set of extracted rules.

---

## Rule redundancy

```yaml
type: FullSlide
key: 3a81639fe1
```

`@part1`
**Comparing extracted rules and non-redundant rules**

```
> inspect(rules) 
    lhs        rhs support   confidence lift     count
[1] {B}     => {A} 0.4285714 0.5        1.166667 3    
[2] {B,D}   => {A} 0.2857143 0.5        1.166667 2    
[3] {B,C,D} => {A} 0.1428571 0.5        1.166667 1 
```

```
> inspect(rules.pruned)
    lhs    rhs support   confidence lift     count
[1] {B} => {A} 0.4285714 0.5        1.166667 3 
```

`@script`
In this example, rule #2 and rule #3 are considered redundant rules as they provide no extra knowledge in addition to rule #1.
"B implies A" is a non-redundant rule given that:
- rules #2 and #3 are super-rules of that rule
- both these rules have a lower lift

---

## Let's practice!

```yaml
type: FinalSlide
key: cbc4d2b5e4
```

`@script`
Now it is your turn to practice the skills you have learned on the Online Retail dataset. Happy shopping!
